{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdds=sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(rdds, rdds.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blue box= Spark Operation\n",
    "black dot= rdd\n",
    "green dot= cached rdd \n",
    "grey stage= cached stage \n",
    "outer red line= stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Stage AKA Narrow Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,\"Goroka\",\"Goroka\",\"Papua New Guinea\",\"GKA\",\"AYGA\",-6.081689,145.391881,5282,10,\"U\",\"Pacific/Port_Moresby\"',\n",
       " '2,\"Madang\",\"Madang\",\"Papua New Guinea\",\"MAG\",\"AYMD\",-5.207083,145.7887,20,10,\"U\",\"Pacific/Port_Moresby\"',\n",
       " '3,\"Mount Hagen\",\"Mount Hagen\",\"Papua New Guinea\",\"HGU\",\"AYMH\",-5.826789,144.295861,5388,10,\"U\",\"Pacific/Port_Moresby\"',\n",
       " '4,\"Nadzab\",\"Nadzab\",\"Papua New Guinea\",\"LAE\",\"AYNZ\",-6.569828,146.726242,239,10,\"U\",\"Pacific/Port_Moresby\"',\n",
       " '5,\"Port Moresby Jacksons Intl\",\"Port Moresby\",\"Papua New Guinea\",\"POM\",\"AYPY\",-9.443383,147.22005,146,10,\"U\",\"Pacific/Port_Moresby\"',\n",
       " '6,\"Wewak Intl\",\"Wewak\",\"Papua New Guinea\",\"WWK\",\"AYWK\",-3.583828,143.669186,19,10,\"U\",\"Pacific/Port_Moresby\"',\n",
       " '7,\"Narsarsuaq\",\"Narssarssuaq\",\"Greenland\",\"UAK\",\"BGBW\",61.160517,-45.425978,112,-3,\"E\",\"America/Godthab\"',\n",
       " '8,\"Nuuk\",\"Godthaab\",\"Greenland\",\"GOH\",\"BGGH\",64.190922,-51.678064,283,-3,\"E\",\"America/Godthab\"',\n",
       " '9,\"Sondre Stromfjord\",\"Sondrestrom\",\"Greenland\",\"SFJ\",\"BGSF\",67.016969,-50.689325,165,-3,\"E\",\"America/Godthab\"']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports = sc.textFile(\"spark_files/airports.txt\")\n",
    "airports.take(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('scala', 1)\n",
      "('java', 1)\n",
      "('hadoop', 1)\n",
      "('spark', 1)\n",
      "('akka', 1)\n",
      "('spark vs hadoop', 1)\n",
      "('pyspark', 1)\n",
      "('Java', 1)\n"
     ]
    }
   ],
   "source": [
    "data = [\"scala\",\"java\",\"hadoop\",\"spark\",\"akka\",\"spark vs hadoop\",\"pyspark\",\"Java\"]\n",
    "rdd=sc.parallelize(data)\n",
    "rdd2=rdd.map(lambda x: (x,1))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Stage or Wide Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mark', 1984)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_one = sc.parallelize([(\"Mark\", 1984), (\"Lisa\", 1985)])\n",
    "ds_two = sc.parallelize([(\"Mark\", 1984), (\"Anastasia\", 2017)])\n",
    "sorted(ds_one.intersection(ds_two).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import*\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, DoubleType\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Test SQL app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"spark_files/RealEstate.csv\", format=\"csv\", header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|      Location|        avg(Price)|\n",
      "+--------------+------------------+\n",
      "|   Pismo Beach| 772374.5833333334|\n",
      "|     King City|          131190.0|\n",
      "|    New Cuyama|           40900.0|\n",
      "|        Nipomo| 454166.6666666667|\n",
      "|        Oceano|          392640.0|\n",
      "|        Nipomo| 430629.4117647059|\n",
      "|     Templeton| 705890.9090909091|\n",
      "| Arroyo Grande|1013958.3333333334|\n",
      "|   Bakersfield|           91500.0|\n",
      "|     Guadalupe|          117250.0|\n",
      "+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Location\", \"Price\").groupby(\"Location\").avg().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "827a1ebbc8f307c8077451cc966d3185efad7db987bf320661a700f88698640d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
